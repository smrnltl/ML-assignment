{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Provincial Economic ML Analysis Pipeline for Nepal\n",
        "==================================================\n",
        "Author: Smaran Luitel\n",
        "\n",
        "Date: Dec 2025"
      ],
      "metadata": {
        "id": "pXsO-nOsJTxS"
      },
      "id": "pXsO-nOsJTxS"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8038fe55",
      "metadata": {
        "id": "8038fe55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "61fd414d",
      "metadata": {
        "id": "61fd414d"
      },
      "outputs": [],
      "source": [
        "# Create output directory for results\n",
        "OUTPUT_DIR = 'outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7c4df02",
      "metadata": {
        "id": "a7c4df02"
      },
      "outputs": [],
      "source": [
        "# Core ML Libraries\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering  # Removed DBSCAN - not used in core analysis\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, StratifiedKFold,\n",
        "    LeaveOneOut, GridSearchCV, cross_validate\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_recall_fscore_support, make_scorer\n",
        ")\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "238ebeae",
      "metadata": {
        "id": "238ebeae"
      },
      "outputs": [],
      "source": [
        "# Advanced ML\n",
        "from sklearn.manifold import TSNE\n",
        "# Removed: from sklearn.mixture import GaussianMixture - not used in core analysis\n",
        "# Removed: import xgboost as xgb - overfits on n=7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f13050a",
      "metadata": {
        "id": "6f13050a"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d9e17c2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2d9e17c2"
      },
      "outputs": [],
      "source": [
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c210e",
      "metadata": {
        "id": "241c210e"
      },
      "source": [
        "PART 1: DATA EXTRACTION AND PREPROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c568d1a7",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c568d1a7"
      },
      "outputs": [],
      "source": [
        "class ProvincialDataExtractor:\n",
        "    \"\"\"Extract and structure provincial GDP data from National Accounts\"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        # Province names in the ORDER they appear in the Excel file\n",
        "        self.province_names = [\n",
        "            'Koshi', 'Madhesh', 'Bagmati', 'Gandaki',\n",
        "            'Lumbini', 'Karnali', 'Sudurpashchim'\n",
        "        ]\n",
        "        self.sectors = {}\n",
        "        self.raw_data = None\n",
        "        self.structured_data = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and parse the Provincial GDP sheet\"\"\"\n",
        "        print(\"Loading Provincial GDP data from National Accounts...\")\n",
        "\n",
        "        # Read the raw data\n",
        "        df = pd.read_excel(self.file_path, sheet_name='Provincial GDP', header=None)\n",
        "        self.raw_data = df\n",
        "\n",
        "        print(f\"Raw data shape: {df.shape}\")\n",
        "\n",
        "        # Find province columns and years\n",
        "        self._identify_structure()\n",
        "\n",
        "        # Extract sector codes and descriptions\n",
        "        self._extract_sectors()\n",
        "\n",
        "        # Create structured dataset\n",
        "        self._create_structured_data()\n",
        "\n",
        "        return self.structured_data\n",
        "\n",
        "    def _identify_structure(self):\n",
        "        \"\"\"Identify the structure of the data sheet\"\"\"\n",
        "        df = self.raw_data\n",
        "\n",
        "        # Find row with years (contains 2075/76, 2076/77, etc.)\n",
        "        self.year_row = None\n",
        "        for i in range(10):\n",
        "            row = df.iloc[i]\n",
        "            if any('2075/76' in str(v) or '2076/77' in str(v) for v in row.values if pd.notna(v)):\n",
        "                self.year_row = i\n",
        "                break\n",
        "\n",
        "        # Find row with province names\n",
        "        self.province_row = None\n",
        "        for i in range(10):\n",
        "            row = df.iloc[i]\n",
        "            if any('Province' in str(v) or 'Madhes' in str(v) or 'Bagmati' in str(v)\n",
        "                   for v in row.values if pd.notna(v)):\n",
        "                self.province_row = i\n",
        "                break\n",
        "\n",
        "        # Data typically starts after the header rows\n",
        "        self.data_start_row = max([r for r in [self.year_row, self.province_row] if r is not None]) + 2\n",
        "\n",
        "        print(f\"Year row: {self.year_row}\")\n",
        "        print(f\"Province row: {self.province_row}\")\n",
        "        print(f\"Data starts at row: {self.data_start_row}\")\n",
        "\n",
        "    def _extract_sectors(self):\n",
        "        \"\"\"Extract sector codes and descriptions\"\"\"\n",
        "        df = self.raw_data\n",
        "\n",
        "        # Sectors are typically in first two columns\n",
        "        for i in range(self.data_start_row, min(self.data_start_row + 30, len(df))):\n",
        "            code = df.iloc[i, 0]\n",
        "            description = df.iloc[i, 1]\n",
        "\n",
        "            if pd.notna(code) and isinstance(code, str) and len(code) <= 2:\n",
        "                if pd.notna(description):\n",
        "                    self.sectors[code] = str(description)\n",
        "\n",
        "        print(f\"Found {len(self.sectors)} economic sectors\")\n",
        "        print(f\"Sectors: {list(self.sectors.keys())}\")\n",
        "\n",
        "    def _create_structured_data(self):\n",
        "        \"\"\"Create structured dataset from raw data\"\"\"\n",
        "        df = self.raw_data\n",
        "\n",
        "        # Find numeric columns (GDP values)\n",
        "        numeric_cols = []\n",
        "        for col in range(2, df.shape[1]):  # Start from column 2\n",
        "            # Check if column has numeric data\n",
        "            sample = pd.to_numeric(df.iloc[self.data_start_row:self.data_start_row+10, col],\n",
        "                                  errors='coerce')\n",
        "            if sample.notna().sum() > 5:\n",
        "                numeric_cols.append(col)\n",
        "\n",
        "        print(f\"Found {len(numeric_cols)} numeric columns\")\n",
        "\n",
        "        # Determine years from row headers\n",
        "        years = []\n",
        "        if self.year_row is not None:\n",
        "            for col in numeric_cols:\n",
        "                year_val = df.iloc[self.year_row, col]\n",
        "                if pd.notna(year_val):\n",
        "                    years.append(str(year_val).strip())\n",
        "\n",
        "        # Create structured data\n",
        "        structured_data = []\n",
        "\n",
        "        # Estimate provinces per year (7 provinces)\n",
        "        n_provinces = 7\n",
        "        cols_per_year = len(numeric_cols) // 8  # 8 years of data expected\n",
        "\n",
        "        # Extract data for each province\n",
        "        for prov_idx, province in enumerate(self.province_names):\n",
        "            province_data = {'Province': province}\n",
        "\n",
        "            # Calculate column range for this province\n",
        "            # Each province has 7 years of data (2075/76 to 2081/82)\n",
        "            start_col = 2 + (prov_idx * 7)  # Each province has 7 years\n",
        "            end_col = start_col + 7\n",
        "\n",
        "            # Get sector values (average across years for stability)\n",
        "            for sector_code in self.sectors.keys():\n",
        "                # Find row for this sector\n",
        "                sector_row = None\n",
        "                for i in range(self.data_start_row, min(self.data_start_row + 30, len(df))):\n",
        "                    if df.iloc[i, 0] == sector_code:\n",
        "                        sector_row = i\n",
        "                        break\n",
        "\n",
        "                if sector_row is not None:\n",
        "                    # Get values for this province across years\n",
        "                    values = []\n",
        "                    for col in range(start_col, min(end_col, df.shape[1])):\n",
        "                        val = pd.to_numeric(df.iloc[sector_row, col], errors='coerce')\n",
        "                        if pd.notna(val):\n",
        "                            values.append(val)\n",
        "\n",
        "                    # Store average value\n",
        "                    if values:\n",
        "                        province_data[f'Sector_{sector_code}'] = np.mean(values)\n",
        "                    else:\n",
        "                        province_data[f'Sector_{sector_code}'] = 0\n",
        "\n",
        "            structured_data.append(province_data)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        self.structured_data = pd.DataFrame(structured_data)\n",
        "        self.structured_data.set_index('Province', inplace=True)\n",
        "\n",
        "        print(f\"Structured data shape: {self.structured_data.shape}\")\n",
        "        print(f\"Provinces: {list(self.structured_data.index)}\")\n",
        "        print(f\"Features: {list(self.structured_data.columns[:5])}...\")\n",
        "\n",
        "        return self.structured_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1efae7b",
      "metadata": {
        "id": "c1efae7b"
      },
      "source": [
        "PART 2: FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9d3e9a44",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "9d3e9a44"
      },
      "outputs": [],
      "source": [
        "class FeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for provincial economic data\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.engineered_features = None\n",
        "\n",
        "    def create_features(self):\n",
        "        \"\"\"Create comprehensive feature set\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"FEATURE ENGINEERING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df = self.data.copy()\n",
        "\n",
        "        # 1. Basic aggregations\n",
        "        df['Total_GDP'] = df.sum(axis=1)\n",
        "        df['Mean_Sector_Value'] = df.mean(axis=1)\n",
        "        df['Median_Sector_Value'] = df.median(axis=1)\n",
        "\n",
        "        # 2. Economic diversity indices\n",
        "        df['Economic_Diversity_HHI'] = self._calculate_hhi(df)\n",
        "        df['Economic_Diversity_Shannon'] = self._calculate_shannon_entropy(df)\n",
        "        df['Economic_Diversity_Gini'] = self._calculate_gini(df)\n",
        "\n",
        "        # 3. Sectoral dominance\n",
        "        df['Max_Sector_Share'] = df.iloc[:, :-6].max(axis=1) / df['Total_GDP']\n",
        "        df['Top3_Sectors_Share'] = self._top_n_share(df, 3)\n",
        "        df['Sector_Concentration'] = df.iloc[:, :-8].std(axis=1) / df['Mean_Sector_Value']\n",
        "\n",
        "        # 4. Economic structure indicators\n",
        "        df['Primary_Sector_Share'] = self._calculate_primary_share(df)\n",
        "        df['Secondary_Sector_Share'] = self._calculate_secondary_share(df)\n",
        "        df['Tertiary_Sector_Share'] = self._calculate_tertiary_share(df)\n",
        "        df['Modernization_Index'] = (df['Secondary_Sector_Share'] +\n",
        "                                     df['Tertiary_Sector_Share'])\n",
        "\n",
        "        # 5. Statistical measures\n",
        "        df['Sector_Variance'] = df.iloc[:, :len(self.data.columns)].var(axis=1)\n",
        "        df['Sector_Skewness'] = df.iloc[:, :len(self.data.columns)].skew(axis=1)\n",
        "        df['Sector_Kurtosis'] = df.iloc[:, :len(self.data.columns)].kurtosis(axis=1)\n",
        "\n",
        "        # 6. Relative measures (compared to national average)\n",
        "        for col in self.data.columns:\n",
        "            if 'Sector_' in col:\n",
        "                nat_avg = df[col].mean()\n",
        "                if nat_avg > 0:\n",
        "                    df[f'{col}_Relative'] = df[col] / nat_avg\n",
        "\n",
        "        self.engineered_features = df\n",
        "\n",
        "        print(f\"Created {len(df.columns) - len(self.data.columns)} new features\")\n",
        "        print(f\"Total features: {len(df.columns)}\")\n",
        "\n",
        "        # Feature importance summary\n",
        "        self._print_feature_summary()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _calculate_hhi(self, df):\n",
        "        \"\"\"Calculate Herfindahl-Hirschman Index\"\"\"\n",
        "        sector_cols = [col for col in df.columns if 'Sector_' in col]\n",
        "        hhi = []\n",
        "        for _, row in df.iterrows():\n",
        "            total = row[sector_cols].sum()\n",
        "            if total > 0:\n",
        "                shares = (row[sector_cols] / total) ** 2\n",
        "                hhi.append(1 - shares.sum())  # 1 - HHI for diversity\n",
        "            else:\n",
        "                hhi.append(0)\n",
        "        return hhi\n",
        "\n",
        "    def _calculate_shannon_entropy(self, df):\n",
        "        \"\"\"Calculate Shannon entropy for diversity\"\"\"\n",
        "        sector_cols = [col for col in df.columns if 'Sector_' in col]\n",
        "        entropy = []\n",
        "        for _, row in df.iterrows():\n",
        "            total = row[sector_cols].sum()\n",
        "            if total > 0:\n",
        "                shares = row[sector_cols] / total\n",
        "                shares = shares[shares > 0]  # Remove zeros\n",
        "                H = -np.sum(shares * np.log(shares))\n",
        "                entropy.append(H)\n",
        "            else:\n",
        "                entropy.append(0)\n",
        "        return entropy\n",
        "\n",
        "    def _calculate_gini(self, df):\n",
        "        \"\"\"Calculate Gini coefficient for inequality\"\"\"\n",
        "        sector_cols = [col for col in df.columns if 'Sector_' in col]\n",
        "        gini = []\n",
        "        for _, row in df.iterrows():\n",
        "            values = row[sector_cols].values\n",
        "            values = values[values > 0]\n",
        "            if len(values) > 0:\n",
        "                sorted_values = np.sort(values)\n",
        "                n = len(sorted_values)\n",
        "                cumsum = np.cumsum(sorted_values)\n",
        "                gini_val = (2 * np.sum((np.arange(1, n+1)) * sorted_values)) / (n * cumsum[-1]) - (n + 1) / n\n",
        "                gini.append(gini_val)\n",
        "            else:\n",
        "                gini.append(0)\n",
        "        return gini\n",
        "\n",
        "    def _top_n_share(self, df, n=3):\n",
        "        \"\"\"Calculate share of top N sectors\"\"\"\n",
        "        sector_cols = [col for col in df.columns if 'Sector_' in col]\n",
        "        shares = []\n",
        "        for _, row in df.iterrows():\n",
        "            total = row[sector_cols].sum()\n",
        "            if total > 0:\n",
        "                top_n = row[sector_cols].nlargest(n).sum()\n",
        "                shares.append(top_n / total)\n",
        "            else:\n",
        "                shares.append(0)\n",
        "        return shares\n",
        "\n",
        "    def _calculate_primary_share(self, df):\n",
        "        \"\"\"Calculate primary sector share (Agriculture, Mining)\"\"\"\n",
        "        primary_sectors = ['Sector_A', 'Sector_B']\n",
        "        available = [col for col in primary_sectors if col in df.columns]\n",
        "        if available:\n",
        "            return df[available].sum(axis=1) / df['Total_GDP']\n",
        "        return pd.Series(0, index=df.index)\n",
        "\n",
        "    def _calculate_secondary_share(self, df):\n",
        "        \"\"\"Calculate secondary sector share (Manufacturing, Construction)\"\"\"\n",
        "        secondary_sectors = ['Sector_C', 'Sector_D', 'Sector_E', 'Sector_F']\n",
        "        available = [col for col in secondary_sectors if col in df.columns]\n",
        "        if available:\n",
        "            return df[available].sum(axis=1) / df['Total_GDP']\n",
        "        return pd.Series(0, index=df.index)\n",
        "\n",
        "    def _calculate_tertiary_share(self, df):\n",
        "        \"\"\"Calculate tertiary sector share (Services)\"\"\"\n",
        "        tertiary_sectors = ['Sector_G', 'Sector_H', 'Sector_I', 'Sector_J',\n",
        "                           'Sector_K', 'Sector_L', 'Sector_M', 'Sector_N']\n",
        "        available = [col for col in tertiary_sectors if col in df.columns]\n",
        "        if available:\n",
        "            return df[available].sum(axis=1) / df['Total_GDP']\n",
        "        return pd.Series(0, index=df.index)\n",
        "\n",
        "    def _print_feature_summary(self):\n",
        "        \"\"\"Print summary of engineered features\"\"\"\n",
        "        print(\"\\nFeature Engineering Summary:\")\n",
        "        print(\"-\" * 40)\n",
        "        feature_groups = {\n",
        "            'Diversity Measures': ['HHI', 'Shannon', 'Gini'],\n",
        "            'Structural Indicators': ['Primary', 'Secondary', 'Tertiary', 'Modernization'],\n",
        "            'Statistical Measures': ['Variance', 'Skewness', 'Kurtosis'],\n",
        "            'Concentration Measures': ['Max_Sector', 'Top3', 'Concentration']\n",
        "        }\n",
        "\n",
        "        for group, keywords in feature_groups.items():\n",
        "            features = [f for f in self.engineered_features.columns\n",
        "                       if any(k in f for k in keywords)]\n",
        "            if features:\n",
        "                print(f\"{group}: {len(features)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645e2735",
      "metadata": {
        "id": "645e2735"
      },
      "source": [
        "PART 3: CLUSTERING ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d70214fb",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "d70214fb"
      },
      "outputs": [],
      "source": [
        "class ProvincialClustering:\n",
        "    \"\"\"Advanced clustering analysis for provincial economies\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaled_data = None\n",
        "        self.clustering_results = {}\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare and scale data for clustering\"\"\"\n",
        "        # Select numeric features\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        self.scaled_data = self.scaler.fit_transform(self.data[numeric_cols])\n",
        "        return self.scaled_data\n",
        "\n",
        "    def find_optimal_clusters(self, max_k=5):\n",
        "        \"\"\"Find optimal number of clusters using multiple metrics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        K_range = range(2, min(max_k + 1, len(self.data)))\n",
        "\n",
        "        metrics = {\n",
        "            'inertia': [],\n",
        "            'silhouette': [],\n",
        "            'davies_bouldin': [],\n",
        "            'calinski_harabasz': []\n",
        "        }\n",
        "\n",
        "        for k in K_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(self.scaled_data)\n",
        "\n",
        "            metrics['inertia'].append(kmeans.inertia_)\n",
        "            metrics['silhouette'].append(silhouette_score(self.scaled_data, labels))\n",
        "            metrics['davies_bouldin'].append(davies_bouldin_score(self.scaled_data, labels))\n",
        "            metrics['calinski_harabasz'].append(calinski_harabasz_score(self.scaled_data, labels))\n",
        "\n",
        "        # Plot metrics\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "        # Elbow curve\n",
        "        axes[0, 0].plot(K_range, metrics['inertia'], 'bo-')\n",
        "        axes[0, 0].set_xlabel('Number of Clusters')\n",
        "        axes[0, 0].set_ylabel('Inertia')\n",
        "        axes[0, 0].set_title('Elbow Method')\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Silhouette score\n",
        "        axes[0, 1].plot(K_range, metrics['silhouette'], 'ro-')\n",
        "        axes[0, 1].set_xlabel('Number of Clusters')\n",
        "        axes[0, 1].set_ylabel('Silhouette Score')\n",
        "        axes[0, 1].set_title('Silhouette Score (Higher is Better)')\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        # Davies-Bouldin score\n",
        "        axes[1, 0].plot(K_range, metrics['davies_bouldin'], 'go-')\n",
        "        axes[1, 0].set_xlabel('Number of Clusters')\n",
        "        axes[1, 0].set_ylabel('Davies-Bouldin Score')\n",
        "        axes[1, 0].set_title('Davies-Bouldin Score (Lower is Better)')\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # Calinski-Harabasz score\n",
        "        axes[1, 1].plot(K_range, metrics['calinski_harabasz'], 'mo-')\n",
        "        axes[1, 1].set_xlabel('Number of Clusters')\n",
        "        axes[1, 1].set_ylabel('Calinski-Harabasz Score')\n",
        "        axes[1, 1].set_title('Calinski-Harabasz Score (Higher is Better)')\n",
        "        axes[1, 1].grid(True)\n",
        "\n",
        "        plt.suptitle('Clustering Optimization Metrics', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'optimal_clusters_analysis.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Determine optimal k\n",
        "        optimal_k = K_range[np.argmax(metrics['silhouette'])]\n",
        "\n",
        "        print(f\"Optimal number of clusters based on Silhouette Score: {optimal_k}\")\n",
        "        print(\"\\nMetrics Summary:\")\n",
        "        for k_idx, k in enumerate(K_range):\n",
        "            print(f\"k={k}: Silhouette={metrics['silhouette'][k_idx]:.3f}, \"\n",
        "                  f\"Davies-Bouldin={metrics['davies_bouldin'][k_idx]:.3f}\")\n",
        "\n",
        "        return optimal_k, metrics\n",
        "\n",
        "    def perform_kmeans(self, n_clusters=3):\n",
        "        \"\"\"Perform K-means clustering\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"K-MEANS CLUSTERING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
        "        labels = kmeans.fit_predict(self.scaled_data)\n",
        "\n",
        "        self.clustering_results['kmeans'] = {\n",
        "            'model': kmeans,\n",
        "            'labels': labels,\n",
        "            'centers': kmeans.cluster_centers_,\n",
        "            'silhouette': silhouette_score(self.scaled_data, labels)\n",
        "        }\n",
        "\n",
        "        # Analyze clusters\n",
        "        self._analyze_clusters(labels, 'K-means')\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def perform_hierarchical(self, n_clusters=3):\n",
        "        \"\"\"Perform Hierarchical clustering\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HIERARCHICAL CLUSTERING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Create linkage matrix\n",
        "        linkage_matrix = linkage(self.scaled_data, method='ward')\n",
        "\n",
        "        # Get clusters\n",
        "        labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust') - 1\n",
        "\n",
        "        self.clustering_results['hierarchical'] = {\n",
        "            'linkage_matrix': linkage_matrix,\n",
        "            'labels': labels,\n",
        "            'silhouette': silhouette_score(self.scaled_data, labels)\n",
        "        }\n",
        "\n",
        "        # Create dendrogram\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        dendrogram(linkage_matrix,\n",
        "                  labels=self.data.index.tolist(),\n",
        "                  leaf_rotation=45,\n",
        "                  leaf_font_size=12)\n",
        "        plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)', fontsize=14)\n",
        "        plt.xlabel('Province')\n",
        "        plt.ylabel('Distance')\n",
        "        plt.axhline(y=linkage_matrix[-n_clusters+1, 2], color='r', linestyle='--',\n",
        "                   label=f'Cut for {n_clusters} clusters')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'hierarchical_dendrogram.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Analyze clusters\n",
        "        self._analyze_clusters(labels, 'Hierarchical')\n",
        "\n",
        "        return labels\n",
        "\n",
        "    # REMOVED: perform_dbscan() - Not used in core analysis\n",
        "    # Density-based clustering not appropriate for n=7 provinces\n",
        "    # Uncomment if needed for additional validation\n",
        "\n",
        "    # REMOVED: perform_gmm() - Not used in core analysis\n",
        "    # GMM adds probabilistic complexity without changing cluster assignments for our data\n",
        "    # Uncomment if needed for additional validation\n",
        "\n",
        "    def _analyze_clusters(self, labels, method_name):\n",
        "        \"\"\"Analyze and characterize clusters\"\"\"\n",
        "        print(f\"\\n{method_name} Cluster Analysis:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for cluster_id in np.unique(labels):\n",
        "            if cluster_id == -1:  # Skip noise points\n",
        "                continue\n",
        "\n",
        "            provinces = self.data.index[labels == cluster_id].tolist()\n",
        "            print(f\"\\nCluster {cluster_id}: {provinces}\")\n",
        "\n",
        "            # Calculate cluster characteristics\n",
        "            cluster_data = self.data.iloc[labels == cluster_id]\n",
        "\n",
        "            if 'Economic_Diversity_HHI' in cluster_data.columns:\n",
        "                avg_diversity = cluster_data['Economic_Diversity_HHI'].mean()\n",
        "                print(f\"  Avg Economic Diversity: {avg_diversity:.3f}\")\n",
        "\n",
        "            if 'Modernization_Index' in cluster_data.columns:\n",
        "                avg_modern = cluster_data['Modernization_Index'].mean()\n",
        "                print(f\"  Avg Modernization Index: {avg_modern:.3f}\")\n",
        "\n",
        "            if 'Total_GDP' in cluster_data.columns:\n",
        "                avg_gdp = cluster_data['Total_GDP'].mean()\n",
        "                print(f\"  Avg Total GDP: {avg_gdp:,.0f}\")\n",
        "\n",
        "    def compare_methods(self):\n",
        "        \"\"\"Compare clustering methods\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"CLUSTERING METHOD COMPARISON\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        comparison_df = pd.DataFrame(index=self.data.index)\n",
        "\n",
        "        for method, results in self.clustering_results.items():\n",
        "            if 'labels' in results:\n",
        "                comparison_df[method] = results['labels']\n",
        "\n",
        "        # Calculate agreement matrix\n",
        "        methods = list(comparison_df.columns)\n",
        "        n_methods = len(methods)\n",
        "        agreement_matrix = np.zeros((n_methods, n_methods))\n",
        "\n",
        "        for i in range(n_methods):\n",
        "            for j in range(n_methods):\n",
        "                if i == j:\n",
        "                    agreement_matrix[i, j] = 1.0\n",
        "                else:\n",
        "                    labels1 = comparison_df[methods[i]].values\n",
        "                    labels2 = comparison_df[methods[j]].values\n",
        "                    # Calculate adjusted rand index\n",
        "                    from sklearn.metrics import adjusted_rand_score\n",
        "                    agreement_matrix[i, j] = adjusted_rand_score(labels1, labels2)\n",
        "\n",
        "        # Plot agreement heatmap\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(agreement_matrix, annot=True, fmt='.2f',\n",
        "                   xticklabels=methods, yticklabels=methods,\n",
        "                   cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
        "        plt.title('Clustering Method Agreement (Adjusted Rand Index)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'clustering_agreement.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\nMethod Agreement Scores:\")\n",
        "        for i in range(n_methods):\n",
        "            for j in range(i+1, n_methods):\n",
        "                print(f\"{methods[i]} vs {methods[j]}: {agreement_matrix[i, j]:.3f}\")\n",
        "\n",
        "        return comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e22658",
      "metadata": {
        "id": "01e22658"
      },
      "source": [
        "PART 4: CLASSIFICATION ANALYSIS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "162cb329",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "162cb329"
      },
      "outputs": [],
      "source": [
        "class ProvincialClassification:\n",
        "    \"\"\"Advanced classification for provincial development tiers\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.classifiers = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def create_labels(self, method='composite'):\n",
        "        \"\"\"Create development tier labels\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"CREATING DEVELOPMENT LABELS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if method == 'composite':\n",
        "            # Create composite development index\n",
        "            scores = pd.Series(index=self.data.index, dtype=float)\n",
        "\n",
        "            # Factors for development score\n",
        "            if 'Total_GDP' in self.data.columns:\n",
        "                gdp_score = (self.data['Total_GDP'] - self.data['Total_GDP'].min()) / \\\n",
        "                           (self.data['Total_GDP'].max() - self.data['Total_GDP'].min() + 1e-10)\n",
        "            else:\n",
        "                gdp_score = pd.Series(0.5, index=self.data.index)\n",
        "\n",
        "            if 'Economic_Diversity_HHI' in self.data.columns:\n",
        "                diversity_score = self.data['Economic_Diversity_HHI']\n",
        "            else:\n",
        "                diversity_score = pd.Series(0.5, index=self.data.index)\n",
        "\n",
        "            if 'Modernization_Index' in self.data.columns:\n",
        "                modern_score = self.data['Modernization_Index']\n",
        "            else:\n",
        "                modern_score = pd.Series(0.5, index=self.data.index)\n",
        "\n",
        "            # Weighted composite\n",
        "            scores = (gdp_score * 0.4 + diversity_score * 0.3 + modern_score * 0.3)\n",
        "\n",
        "            # Create labels based on tertiles\n",
        "            tertiles = scores.quantile([0.33, 0.67])\n",
        "\n",
        "            labels = pd.Series(index=self.data.index, dtype=str)\n",
        "            labels[scores <= tertiles.iloc[0]] = 'Lagging'\n",
        "            labels[(scores > tertiles.iloc[0]) & (scores <= tertiles.iloc[1])] = 'Emerging'\n",
        "            labels[scores > tertiles.iloc[1]] = 'Advanced'\n",
        "\n",
        "        else:\n",
        "            # Alternative: use clustering results\n",
        "            if 'kmeans' in self.clustering_results:\n",
        "                cluster_labels = self.clustering_results['kmeans']['labels']\n",
        "                label_map = {0: 'Type_A', 1: 'Type_B', 2: 'Type_C'}\n",
        "                labels = pd.Series([label_map.get(l, 'Unknown') for l in cluster_labels],\n",
        "                                 index=self.data.index)\n",
        "\n",
        "        self.y = labels\n",
        "\n",
        "        print(\"\\nDevelopment Tier Distribution:\")\n",
        "        print(labels.value_counts())\n",
        "\n",
        "        for tier in labels.unique():\n",
        "            provinces = labels[labels == tier].index.tolist()\n",
        "            print(f\"\\n{tier}: {provinces}\")\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def prepare_features(self, feature_selection=True):\n",
        "        \"\"\"Prepare features for classification\"\"\"\n",
        "        # Select numeric features\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        self.X = self.data[numeric_cols]\n",
        "\n",
        "        if feature_selection and len(self.X.columns) > 10:\n",
        "            # Feature selection using Random Forest importance\n",
        "            print(\"\\nPerforming feature selection...\")\n",
        "\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            y_encoded = le.fit_transform(self.y)\n",
        "\n",
        "            # Use Random Forest for feature importance\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            rf.fit(self.X, y_encoded)\n",
        "\n",
        "            # Get feature importance\n",
        "            importance = pd.DataFrame({\n",
        "                'feature': self.X.columns,\n",
        "                'importance': rf.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            # Select top features\n",
        "            n_features = min(10, len(self.X.columns) // 2)\n",
        "            top_features = importance.head(n_features)['feature'].tolist()\n",
        "\n",
        "            print(f\"Selected {n_features} top features\")\n",
        "            print(\"Top 5 features:\")\n",
        "            for idx, row in importance.head(5).iterrows():\n",
        "                print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "            self.X = self.X[top_features]\n",
        "\n",
        "        return self.X\n",
        "\n",
        "    def train_classifiers(self):\n",
        "        \"\"\"Train multiple classifiers\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"TRAINING CLASSIFIERS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Encode labels\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(self.y)\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(self.X)\n",
        "\n",
        "        # Define classifiers - SIMPLIFIED to 2 core algorithms\n",
        "        classifiers = {\n",
        "            'Random Forest': RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=3,\n",
        "                min_samples_split=2,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(\n",
        "                n_estimators=50,\n",
        "                max_depth=2,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42\n",
        "            )\n",
        "            # REMOVED: XGBoost - overfits on n=7 (achieved only 14.3% accuracy)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each classifier\n",
        "        for name, clf in classifiers.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            # Use Leave-One-Out cross-validation for small sample\n",
        "            loo = LeaveOneOut()\n",
        "            scores = cross_val_score(clf, X_scaled, y_encoded, cv=loo)\n",
        "\n",
        "            # Train on full dataset\n",
        "            clf.fit(X_scaled, y_encoded)\n",
        "\n",
        "            # Store results\n",
        "            self.classifiers[name] = {\n",
        "                'model': clf,\n",
        "                'scaler': scaler,\n",
        "                'label_encoder': le,\n",
        "                'cv_scores': scores,\n",
        "                'mean_accuracy': scores.mean(),\n",
        "                'std_accuracy': scores.std()\n",
        "            }\n",
        "\n",
        "            print(f\"  LOO CV Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
        "\n",
        "            # Feature importance for tree-based models\n",
        "            if hasattr(clf, 'feature_importances_'):\n",
        "                importance = pd.DataFrame({\n",
        "                    'feature': self.X.columns,\n",
        "                    'importance': clf.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                print(f\"  Top 3 important features:\")\n",
        "                for idx, row in importance.head(3).iterrows():\n",
        "                    print(f\"    {row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "        return self.classifiers\n",
        "\n",
        "    def create_ensemble(self):\n",
        "        \"\"\"Create ensemble classifier\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ENSEMBLE CLASSIFIER\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Create voting classifier\n",
        "        estimators = []\n",
        "        for name, clf_dict in self.classifiers.items():\n",
        "            estimators.append((name, clf_dict['model']))\n",
        "\n",
        "        ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
        "\n",
        "        # Encode labels\n",
        "        le = self.classifiers['Random Forest']['label_encoder']\n",
        "        y_encoded = le.transform(self.y)\n",
        "\n",
        "        # Scale features\n",
        "        scaler = self.classifiers['Random Forest']['scaler']\n",
        "        X_scaled = scaler.transform(self.X)\n",
        "\n",
        "        # Cross-validate ensemble\n",
        "        loo = LeaveOneOut()\n",
        "        scores = cross_val_score(ensemble, X_scaled, y_encoded, cv=loo)\n",
        "\n",
        "        # Train ensemble\n",
        "        ensemble.fit(X_scaled, y_encoded)\n",
        "\n",
        "        self.classifiers['Ensemble'] = {\n",
        "            'model': ensemble,\n",
        "            'scaler': scaler,\n",
        "            'label_encoder': le,\n",
        "            'cv_scores': scores,\n",
        "            'mean_accuracy': scores.mean(),\n",
        "            'std_accuracy': scores.std()\n",
        "        }\n",
        "\n",
        "        print(f\"Ensemble LOO CV Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
        "\n",
        "        return ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05651e31",
      "metadata": {
        "id": "05651e31"
      },
      "source": [
        "PART 5: VISUALIZATION AND REPORTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "097a275b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "097a275b"
      },
      "outputs": [],
      "source": [
        "class VisualizationReporter:\n",
        "    \"\"\"Create comprehensive visualizations and reports\"\"\"\n",
        "\n",
        "    def __init__(self, data, clustering_results, classification_results):\n",
        "        self.data = data\n",
        "        self.clustering_results = clustering_results\n",
        "        self.classification_results = classification_results\n",
        "\n",
        "    def create_pca_visualization(self):\n",
        "        \"\"\"Create PCA visualization of clusters\"\"\"\n",
        "        print(\"\\nCreating PCA visualization...\")\n",
        "\n",
        "        # Prepare data\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(self.data[numeric_cols])\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "        # Create plots for each clustering method\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # Changed from 2x2 to 1x2 for 2 methods\n",
        "        methods = ['kmeans', 'hierarchical']  # SIMPLIFIED - only core methods\n",
        "\n",
        "        for idx, method in enumerate(methods):\n",
        "            ax = axes[idx]\n",
        "\n",
        "            if method in self.clustering_results and 'labels' in self.clustering_results[method]:\n",
        "                labels = self.clustering_results[method]['labels']\n",
        "\n",
        "                # Plot\n",
        "                scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1],\n",
        "                                   c=labels, cmap='viridis',\n",
        "                                   s=200, alpha=0.7, edgecolors='black')\n",
        "\n",
        "                # Add province names\n",
        "                for i, province in enumerate(self.data.index):\n",
        "                    ax.annotate(province.replace('Province ', 'P'),\n",
        "                              (X_pca[i, 0], X_pca[i, 1]),\n",
        "                              ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "                ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "                ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "                ax.set_title(f'{method.upper()} Clustering')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, f'{method.upper()}\\nNot Available',\n",
        "                       ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_xticks([])\n",
        "                ax.set_yticks([])\n",
        "\n",
        "        plt.suptitle('Provincial Clusters - PCA Visualization', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'pca_clusters_comparison.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_tsne_visualization(self):\n",
        "        \"\"\"Create t-SNE visualization\"\"\"\n",
        "        print(\"Creating t-SNE visualization...\")\n",
        "\n",
        "        # Prepare data\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(self.data[numeric_cols])\n",
        "\n",
        "        # t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=3)\n",
        "        X_tsne = tsne.fit_transform(scaled_data)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        if 'kmeans' in self.clustering_results:\n",
        "            labels = self.clustering_results['kmeans']['labels']\n",
        "            scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
        "                                c=labels, cmap='Set1',\n",
        "                                s=300, alpha=0.7, edgecolors='black')\n",
        "\n",
        "            # Add province names\n",
        "            for i, province in enumerate(self.data.index):\n",
        "                plt.annotate(province, (X_tsne[i, 0], X_tsne[i, 1]),\n",
        "                           ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "            plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "        plt.xlabel('t-SNE Component 1')\n",
        "        plt.ylabel('t-SNE Component 2')\n",
        "        plt.title('Provincial Economic Structure - t-SNE Visualization')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'tsne_visualization.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_comprehensive_report(self):\n",
        "        \"\"\"Create comprehensive analysis report\"\"\"\n",
        "        print(\"\\nGenerating comprehensive report...\")\n",
        "\n",
        "        # Create summary dashboard\n",
        "        fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "        # 1. Cluster sizes\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        if 'kmeans' in self.clustering_results:\n",
        "            labels = self.clustering_results['kmeans']['labels']\n",
        "            unique, counts = np.unique(labels, return_counts=True)\n",
        "            ax1.bar(unique, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "            ax1.set_xlabel('Cluster')\n",
        "            ax1.set_ylabel('Number of Provinces')\n",
        "            ax1.set_title('K-means Cluster Distribution')\n",
        "            ax1.set_xticks(unique)\n",
        "\n",
        "        # 2. Development tier distribution\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        if self.classification_results and hasattr(self.classification_results, 'y'):\n",
        "            tier_counts = self.classification_results.y.value_counts()\n",
        "            colors = {'Advanced': 'green', 'Emerging': 'yellow', 'Lagging': 'red'}\n",
        "            bar_colors = [colors.get(x, 'gray') for x in tier_counts.index]\n",
        "            ax2.bar(tier_counts.index, tier_counts.values, color=bar_colors)\n",
        "            ax2.set_xlabel('Development Tier')\n",
        "            ax2.set_ylabel('Count')\n",
        "            ax2.set_title('Provincial Development Classification')\n",
        "            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
        "\n",
        "        # 3. Economic diversity comparison\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        if 'Economic_Diversity_HHI' in self.data.columns:\n",
        "            provinces = self.data.index\n",
        "            diversity = self.data['Economic_Diversity_HHI'].values\n",
        "            ax3.barh(range(len(provinces)), diversity)\n",
        "            ax3.set_yticks(range(len(provinces)))\n",
        "            ax3.set_yticklabels([p.replace('Province ', 'P') for p in provinces])\n",
        "            ax3.set_xlabel('Economic Diversity (HHI)')\n",
        "            ax3.set_title('Provincial Economic Diversity')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Feature importance\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        if self.classification_results and 'Random Forest' in self.classification_results.classifiers:\n",
        "            rf = self.classification_results.classifiers['Random Forest']['model']\n",
        "            if hasattr(rf, 'feature_importances_'):\n",
        "                importance = pd.DataFrame({\n",
        "                    'feature': self.classification_results.X.columns,\n",
        "                    'importance': rf.feature_importances_\n",
        "                }).sort_values('importance', ascending=True).tail(8)\n",
        "\n",
        "                ax4.barh(range(len(importance)), importance['importance'].values)\n",
        "                ax4.set_yticks(range(len(importance)))\n",
        "                ax4.set_yticklabels([f[:20] for f in importance['feature'].values])\n",
        "                ax4.set_xlabel('Importance')\n",
        "                ax4.set_title('Top Feature Importance (RF)')\n",
        "\n",
        "        # 5. Clustering agreement matrix\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        methods_comparison = []\n",
        "        methods_names = []\n",
        "        for method in ['kmeans', 'hierarchical']:  # SIMPLIFIED - only core methods\n",
        "            if method in self.clustering_results and 'labels' in self.clustering_results[method]:\n",
        "                methods_comparison.append(self.clustering_results[method]['labels'])\n",
        "                methods_names.append(method)\n",
        "\n",
        "        if len(methods_comparison) >= 2:\n",
        "            from sklearn.metrics import adjusted_rand_score\n",
        "            n_methods = len(methods_comparison)\n",
        "            agreement = np.zeros((n_methods, n_methods))\n",
        "\n",
        "            for i in range(n_methods):\n",
        "                for j in range(n_methods):\n",
        "                    agreement[i, j] = adjusted_rand_score(methods_comparison[i],\n",
        "                                                         methods_comparison[j])\n",
        "\n",
        "            im = ax5.imshow(agreement, cmap='coolwarm', vmin=0, vmax=1)\n",
        "            ax5.set_xticks(range(n_methods))\n",
        "            ax5.set_yticks(range(n_methods))\n",
        "            ax5.set_xticklabels(methods_names, rotation=45)\n",
        "            ax5.set_yticklabels(methods_names)\n",
        "            ax5.set_title('Clustering Agreement (ARI)')\n",
        "\n",
        "            # Add values\n",
        "            for i in range(n_methods):\n",
        "                for j in range(n_methods):\n",
        "                    ax5.text(j, i, f'{agreement[i, j]:.2f}',\n",
        "                           ha='center', va='center')\n",
        "\n",
        "        # 6. Modernization vs GDP\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        if 'Modernization_Index' in self.data.columns and 'Total_GDP' in self.data.columns:\n",
        "            ax6.scatter(self.data['Modernization_Index'],\n",
        "                       self.data['Total_GDP'],\n",
        "                       s=200, alpha=0.7)\n",
        "\n",
        "            for i, province in enumerate(self.data.index):\n",
        "                ax6.annotate(province.replace('Province ', 'P'),\n",
        "                           (self.data['Modernization_Index'].iloc[i],\n",
        "                            self.data['Total_GDP'].iloc[i]),\n",
        "                           ha='center', va='bottom')\n",
        "\n",
        "            ax6.set_xlabel('Modernization Index')\n",
        "            ax6.set_ylabel('Total GDP')\n",
        "            ax6.set_title('Modernization vs Economic Size')\n",
        "            ax6.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Silhouette scores comparison\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        silhouette_scores = []\n",
        "        method_names = []\n",
        "        for method, results in self.clustering_results.items():\n",
        "            if 'silhouette' in results and results['silhouette'] > -1:\n",
        "                silhouette_scores.append(results['silhouette'])\n",
        "                method_names.append(method)\n",
        "\n",
        "        if silhouette_scores:\n",
        "            ax7.bar(method_names, silhouette_scores,\n",
        "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "            ax7.set_ylabel('Silhouette Score')\n",
        "            ax7.set_title('Clustering Quality Comparison')\n",
        "            ax7.set_ylim([0, 1])\n",
        "            ax7.grid(True, alpha=0.3)\n",
        "\n",
        "            # Add value labels\n",
        "            for i, v in enumerate(silhouette_scores):\n",
        "                ax7.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "        # 8. Classification accuracy comparison\n",
        "        ax8 = plt.subplot(3, 3, 8)\n",
        "        if self.classification_results and self.classification_results.classifiers:\n",
        "            accuracies = []\n",
        "            classifier_names = []\n",
        "\n",
        "            for name, clf_dict in self.classification_results.classifiers.items():\n",
        "                accuracies.append(clf_dict['mean_accuracy'])\n",
        "                classifier_names.append(name)\n",
        "\n",
        "            ax8.bar(classifier_names, accuracies,\n",
        "                   color=['#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#9467bd'])\n",
        "            ax8.set_ylabel('LOO CV Accuracy')\n",
        "            ax8.set_title('Classifier Performance Comparison')\n",
        "            ax8.set_ylim([0, 1])\n",
        "            plt.setp(ax8.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "            ax8.grid(True, alpha=0.3)\n",
        "\n",
        "            # Add value labels\n",
        "            for i, v in enumerate(accuracies):\n",
        "                ax8.text(i, v + 0.01, f'{v:.2%}', ha='center')\n",
        "\n",
        "        # 9. Sectoral composition heatmap\n",
        "        ax9 = plt.subplot(3, 3, 9)\n",
        "        sector_cols = [col for col in self.data.columns if col.startswith('Sector_')][:8]\n",
        "        if sector_cols:\n",
        "            sector_data = self.data[sector_cols].T\n",
        "            sector_data.index = [s.replace('Sector_', '') for s in sector_data.index]\n",
        "            sector_data.columns = [p.replace('Province ', 'P') for p in sector_data.columns]\n",
        "\n",
        "            # Normalize by column (province)\n",
        "            sector_norm = sector_data.div(sector_data.sum(axis=0), axis=1)\n",
        "\n",
        "            im = ax9.imshow(sector_norm.values, cmap='YlOrRd', aspect='auto')\n",
        "            ax9.set_xticks(range(len(sector_data.columns)))\n",
        "            ax9.set_yticks(range(len(sector_data.index)))\n",
        "            ax9.set_xticklabels(sector_data.columns)\n",
        "            ax9.set_yticklabels(sector_data.index)\n",
        "            ax9.set_title('Sectoral Composition (Normalized)')\n",
        "            ax9.set_xlabel('Province')\n",
        "            ax9.set_ylabel('Sector')\n",
        "\n",
        "        plt.suptitle('Nepal Provincial Economic ML Analysis - Comprehensive Dashboard',\n",
        "                    fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'comprehensive_dashboard.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Dashboard created successfully!\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save all results to files\"\"\"\n",
        "        print(\"\\nSaving results...\")\n",
        "\n",
        "        # Create results dataframe\n",
        "        results_df = self.data.copy()\n",
        "\n",
        "        # Add clustering results\n",
        "        for method, results in self.clustering_results.items():\n",
        "            if 'labels' in results:\n",
        "                results_df[f'cluster_{method}'] = results['labels']\n",
        "\n",
        "        # Add classification results\n",
        "        if self.classification_results and hasattr(self.classification_results, 'y'):\n",
        "            results_df['development_tier'] = self.classification_results.y\n",
        "\n",
        "        # Save to CSV\n",
        "        results_df.to_csv(os.path.join(OUTPUT_DIR, 'provincial_analysis_results.csv'))\n",
        "\n",
        "        # Save summary statistics\n",
        "        summary = {\n",
        "            'n_provinces': len(self.data),\n",
        "            'n_features': len(self.data.columns),\n",
        "            'clustering_methods': list(self.clustering_results.keys()),\n",
        "            'classification_methods': list(self.classification_results.classifiers.keys())\n",
        "                                    if self.classification_results else [],\n",
        "            'best_clustering_silhouette': max(\n",
        "                [r.get('silhouette', -1) for r in self.clustering_results.values()]\n",
        "            ),\n",
        "            'best_classification_accuracy': max(\n",
        "                [c['mean_accuracy'] for c in self.classification_results.classifiers.values()]\n",
        "            ) if self.classification_results else 0\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(OUTPUT_DIR, 'analysis_summary.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "        print(\"Results saved successfully!\")\n",
        "        print(f\"  - Results CSV: {os.path.join(OUTPUT_DIR, 'provincial_analysis_results.csv')}\")\n",
        "        print(f\"  - Summary JSON: {os.path.join(OUTPUT_DIR, 'analysis_summary.json')}\")\n",
        "        print(f\"  - Visualizations: {OUTPUT_DIR}/*.png files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1480b8cf",
      "metadata": {
        "id": "1480b8cf"
      },
      "source": [
        "MAIN EXECUTION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "df90cabb",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "df90cabb"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Execute complete ML pipeline\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"NEPAL PROVINCIAL ECONOMIC ML ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Extract Data\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: DATA EXTRACTION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Use relative path to data file\n",
        "    data_path =  'National-Accounts.xlsx'#os.path.join('data', 'National-Accounts.xlsx')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"ERROR: Data file not found at {data_path}\")\n",
        "        print(\"Please ensure National-Accounts.xlsx is in the 'data' directory\")\n",
        "        return\n",
        "\n",
        "    extractor = ProvincialDataExtractor(data_path)\n",
        "    raw_data = extractor.load_data()\n",
        "\n",
        "    if raw_data is None or raw_data.empty:\n",
        "        print(\"ERROR: Could not extract data. Please check the file structure.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Feature Engineering\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 2: FEATURE ENGINEERING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    engineer = FeatureEngineer(raw_data)\n",
        "    engineered_data = engineer.create_features()\n",
        "\n",
        "    # Step 3: Clustering Analysis\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 3: CLUSTERING ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    clustering = ProvincialClustering(engineered_data)\n",
        "    clustering.prepare_data()\n",
        "\n",
        "    # Find optimal clusters\n",
        "    optimal_k, metrics = clustering.find_optimal_clusters(max_k=4)\n",
        "\n",
        "    # Perform multiple clustering methods\n",
        "    # CORE CLUSTERING ALGORITHMS (Simplified from 4 to 2)\n",
        "    clustering.perform_kmeans(n_clusters=optimal_k)\n",
        "    clustering.perform_hierarchical(n_clusters=optimal_k)\n",
        "    # REMOVED: clustering.perform_gmm() and clustering.perform_dbscan() - not in core analysis\n",
        "\n",
        "    # Compare methods\n",
        "    comparison = clustering.compare_methods()\n",
        "\n",
        "    # Step 4: Classification Analysis\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 4: CLASSIFICATION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    classification = ProvincialClassification(engineered_data)\n",
        "    classification.clustering_results = clustering.clustering_results  # Pass clustering results\n",
        "\n",
        "    # Create labels\n",
        "    classification.create_labels(method='composite')\n",
        "\n",
        "    # Prepare features\n",
        "    classification.prepare_features(feature_selection=True)\n",
        "\n",
        "    # Train classifiers\n",
        "    classification.train_classifiers()\n",
        "\n",
        "    # Create ensemble\n",
        "    classification.create_ensemble()\n",
        "\n",
        "    # Step 5: Visualization and Reporting\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 5: VISUALIZATION AND REPORTING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    reporter = VisualizationReporter(engineered_data,\n",
        "                                    clustering.clustering_results,\n",
        "                                    classification)\n",
        "\n",
        "    reporter.create_pca_visualization()\n",
        "    reporter.create_tsne_visualization()\n",
        "    reporter.create_comprehensive_report()\n",
        "    reporter.save_results()\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nKey Findings:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Best clustering method\n",
        "    best_clustering = max(clustering.clustering_results.items(),\n",
        "                         key=lambda x: x[1].get('silhouette', -1))\n",
        "    print(f\"Best Clustering Method: {best_clustering[0].upper()}\")\n",
        "    print(f\"  Silhouette Score: {best_clustering[1]['silhouette']:.3f}\")\n",
        "\n",
        "    # Best classifier\n",
        "    if classification.classifiers:\n",
        "        best_classifier = max(classification.classifiers.items(),\n",
        "                            key=lambda x: x[1]['mean_accuracy'])\n",
        "        print(f\"\\nBest Classifier: {best_classifier[0]}\")\n",
        "        print(f\"  LOO CV Accuracy: {best_classifier[1]['mean_accuracy']:.1%}\")\n",
        "\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(\"  1. optimal_clusters_analysis.png\")\n",
        "    print(\"  2. hierarchical_dendrogram.png\")\n",
        "    print(\"  3. clustering_agreement.png\")\n",
        "    print(\"  4. pca_clusters_comparison.png\")\n",
        "    print(\"  5. tsne_visualization.png\")\n",
        "    print(\"  6. comprehensive_dashboard.png\")\n",
        "    print(\"  7. provincial_analysis_results.csv\")\n",
        "    print(\"  8. analysis_summary.json\")\n",
        "\n",
        "    # Step 6: Generate IEEE Publication-Quality Images\n",
        "    # print(\"\\n\" + \"=\"*50)\n",
        "    # print(\"STEP 6: GENERATING IEEE PAPER IMAGES\")\n",
        "    # print(\"=\"*50)\n",
        "\n",
        "    # try:\n",
        "    #     import subprocess\n",
        "    #     result = subprocess.run(['python', 'create_paper_images.py'],\n",
        "    #                           capture_output=True, text=True, check=True)\n",
        "    #     print(result.stdout)\n",
        "    #     print(\"\\n[SUCCESS] IEEE-compliant images generated for academic paper!\")\n",
        "    # except subprocess.CalledProcessError as e:\n",
        "    #     print(f\"[WARNING] Could not generate IEEE images: {e}\")\n",
        "    #     print(\"You can manually run: python create_paper_images.py\")\n",
        "    # except FileNotFoundError:\n",
        "    #     print(\"[WARNING] create_paper_images.py not found\")\n",
        "    #     print(\"IEEE images will not be generated automatically\")\n",
        "\n",
        "    return clustering, classification, reporter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5838050e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5838050e",
        "outputId": "6c70b4fc-4f63-4539-9f35-b92eb3c78b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NEPAL PROVINCIAL ECONOMIC ML ANALYSIS\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "STEP 1: DATA EXTRACTION\n",
            "==================================================\n",
            "Loading Provincial GDP data from National Accounts...\n",
            "Raw data shape: (168, 59)\n",
            "Year row: 4\n",
            "Province row: 3\n",
            "Data starts at row: 6\n",
            "Found 17 economic sectors\n",
            "Sectors: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q']\n",
            "Found 56 numeric columns\n",
            "Structured data shape: (7, 17)\n",
            "Provinces: ['Koshi', 'Madhesh', 'Bagmati', 'Gandaki', 'Lumbini', 'Karnali', 'Sudurpashchim']\n",
            "Features: ['Sector_A', 'Sector_B', 'Sector_C', 'Sector_D', 'Sector_E']...\n",
            "\n",
            "==================================================\n",
            "STEP 2: FEATURE ENGINEERING\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "FEATURE ENGINEERING\n",
            "==================================================\n",
            "Created 33 new features\n",
            "Total features: 50\n",
            "\n",
            "Feature Engineering Summary:\n",
            "----------------------------------------\n",
            "Diversity Measures: 3 features\n",
            "Structural Indicators: 4 features\n",
            "Statistical Measures: 3 features\n",
            "Concentration Measures: 3 features\n",
            "\n",
            "==================================================\n",
            "STEP 3: CLUSTERING ANALYSIS\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "FINDING OPTIMAL NUMBER OF CLUSTERS\n",
            "==================================================\n",
            "Optimal number of clusters based on Silhouette Score: 2\n",
            "\n",
            "Metrics Summary:\n",
            "k=2: Silhouette=0.502, Davies-Bouldin=0.278\n",
            "k=3: Silhouette=0.301, Davies-Bouldin=0.579\n",
            "k=4: Silhouette=0.295, Davies-Bouldin=0.417\n",
            "\n",
            "==================================================\n",
            "K-MEANS CLUSTERING\n",
            "==================================================\n",
            "\n",
            "K-means Cluster Analysis:\n",
            "----------------------------------------\n",
            "\n",
            "Cluster 0: ['Bagmati']\n",
            "  Avg Economic Diversity: 0.897\n",
            "  Avg Modernization Index: 0.772\n",
            "  Avg Total GDP: 1,550,187\n",
            "\n",
            "Cluster 1: ['Koshi', 'Madhesh', 'Gandaki', 'Lumbini', 'Karnali', 'Sudurpashchim']\n",
            "  Avg Economic Diversity: 0.866\n",
            "  Avg Modernization Index: 0.411\n",
            "  Avg Total GDP: 449,964\n",
            "\n",
            "==================================================\n",
            "HIERARCHICAL CLUSTERING\n",
            "==================================================\n",
            "\n",
            "Hierarchical Cluster Analysis:\n",
            "----------------------------------------\n",
            "\n",
            "Cluster 0: ['Koshi', 'Madhesh', 'Gandaki', 'Lumbini', 'Karnali', 'Sudurpashchim']\n",
            "  Avg Economic Diversity: 0.866\n",
            "  Avg Modernization Index: 0.411\n",
            "  Avg Total GDP: 449,964\n",
            "\n",
            "Cluster 1: ['Bagmati']\n",
            "  Avg Economic Diversity: 0.897\n",
            "  Avg Modernization Index: 0.772\n",
            "  Avg Total GDP: 1,550,187\n",
            "\n",
            "==================================================\n",
            "CLUSTERING METHOD COMPARISON\n",
            "==================================================\n",
            "\n",
            "Method Agreement Scores:\n",
            "kmeans vs hierarchical: 1.000\n",
            "\n",
            "==================================================\n",
            "STEP 4: CLASSIFICATION ANALYSIS\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "CREATING DEVELOPMENT LABELS\n",
            "==================================================\n",
            "\n",
            "Development Tier Distribution:\n",
            "Emerging    3\n",
            "Advanced    2\n",
            "Lagging     2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Advanced: ['Koshi', 'Bagmati']\n",
            "\n",
            "Emerging: ['Madhesh', 'Gandaki', 'Lumbini']\n",
            "\n",
            "Lagging: ['Karnali', 'Sudurpashchim']\n",
            "\n",
            "Performing feature selection...\n",
            "Selected 10 top features\n",
            "Top 5 features:\n",
            "  Sector_L_Relative: 0.050\n",
            "  Sector_M_Relative: 0.047\n",
            "  Sector_F: 0.041\n",
            "  Sector_J: 0.037\n",
            "  Sector_Q_Relative: 0.037\n",
            "\n",
            "==================================================\n",
            "TRAINING CLASSIFIERS\n",
            "==================================================\n",
            "\n",
            "Training Random Forest...\n",
            "  LOO CV Accuracy: 0.714 (+/- 0.452)\n",
            "  Top 3 important features:\n",
            "    Sector_L_Relative: 0.141\n",
            "    Sector_N_Relative: 0.116\n",
            "    Sector_J: 0.104\n",
            "\n",
            "Training Gradient Boosting...\n",
            "  LOO CV Accuracy: 0.571 (+/- 0.495)\n",
            "  Top 3 important features:\n",
            "    Sector_Q_Relative: 0.258\n",
            "    Sector_N_Relative: 0.178\n",
            "    Sector_M: 0.157\n",
            "\n",
            "==================================================\n",
            "ENSEMBLE CLASSIFIER\n",
            "==================================================\n",
            "Ensemble LOO CV Accuracy: 0.571 (+/- 0.495)\n",
            "\n",
            "==================================================\n",
            "STEP 5: VISUALIZATION AND REPORTING\n",
            "==================================================\n",
            "\n",
            "Creating PCA visualization...\n",
            "Creating t-SNE visualization...\n",
            "\n",
            "Generating comprehensive report...\n",
            "Dashboard created successfully!\n",
            "\n",
            "Saving results...\n",
            "Results saved successfully!\n",
            "  - Results CSV: outputs/provincial_analysis_results.csv\n",
            "  - Summary JSON: outputs/analysis_summary.json\n",
            "  - Visualizations: outputs/*.png files\n",
            "\n",
            "============================================================\n",
            "ANALYSIS COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Key Findings:\n",
            "----------------------------------------\n",
            "Best Clustering Method: KMEANS\n",
            "  Silhouette Score: 0.502\n",
            "\n",
            "Best Classifier: Random Forest\n",
            "  LOO CV Accuracy: 71.4%\n",
            "\n",
            "Generated Files:\n",
            "  1. optimal_clusters_analysis.png\n",
            "  2. hierarchical_dendrogram.png\n",
            "  3. clustering_agreement.png\n",
            "  4. pca_clusters_comparison.png\n",
            "  5. tsne_visualization.png\n",
            "  6. comprehensive_dashboard.png\n",
            "  7. provincial_analysis_results.csv\n",
            "  8. analysis_summary.json\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    clustering, classification, reporter = main()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "executable": "/usr/bin/env python3",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}